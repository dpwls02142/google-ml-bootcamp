# 3주차


### 📌 신경망

- **신경망이란:** 인간의 두뇌에서 영감을 받아 설계된 인공지능 시스템.
- **구성 요소**
    - **뉴런(=노드, 유닛)**: 신경망의 기본 단위.
    - **입력층**: 외부 데이터가 입력되는 층으로 각 노드는 하나의 입력 특성을 나타냄.
    - **은닉층**: 입력층과 출력층 사이에 위치한 레이어로, 여러개로 구성될 수 있음. 은닉층의 노드들은 활성화 함수를 활용하여 이전 층의 출력을 입력으로 받아 가공하여 다음층으로 전달함.
    - **출력층**: 최종 예측값을 생성하는 층.
- 즉 아래 계산 그래프 그림[그림1]을 예시로, 작은 시그모이드 유닛을 여러개 쌓아 신경망을 형성할 수 있음.
<p align="center"><img src="https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/ab1efab4-fdd7-4103-9334-597588e17d60"></p>
<p align="center">[그림1] 계산 그래프</p>

<p align="center"> <img src="https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/9da3a155-5176-4f50-947e-b73bfcd3a1b5"></p>
<p align="center">[그림2] [그림1]을 활용한 신경망 </p>

- 위 그림[그림 2]은 [그림1]을 여러개 쌓아 만든 신경망을 나타내는 그림임. 시그모이드 유닛을 여러개 쌓아 만들었으니, 그만큼 레이어도 여러개 존재할거임. 그러니 레이어의 순서를 구별해주는게 좋겠지. 그래서 윗첨자를 활용해 레이어의 순서를 나타낼 수 있음. 즉, $z^{[1]}$은 첫번째 레이어에 위치한 뉴런이란 의미임.

수식으로 정리하면, 아래와 같음.

![image](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/21ed6800-5f99-40fc-9d8f-db6fc9493a8f)

- $Z^{[1]}_1$이란건, 첫번째 히든레이어의 첫번째 뉴런 활성화 값(weight(W)는 곱하고 bias(b)는 더한값)을 의미함. 즉, **윗첨자는 앞에서 말했듯 레이어의 순서를 나타내고, 아래첨자는 노드의 순서라는 걸 알 수 있음.**
- $X$는 입력 특징 벡터를 의미함. 따라서 $A^{[0]}$으로도 나타낼 수 있음. 왜? $A^{[0]}$라는건 첫번째 레이어를 의미하니까.
- $A^{[1]}_1$는 첫번째 히든레이어의 첫번째 뉴런이 활성화 함수를 거친 결과임. $Z^{[1]}_1$에 활성화 함수 $\sigma$를 씌운 값이니, 이 값은 다음 레이어에 전달 되는 값이란걸 알 수 있음.

### 📌 활성화 함수

![Untitled 2](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/8db1b301-11cc-46bf-ade6-f698299888f6)

- **시그모이드 (Sigmoid)**
    - 출력범위: 0~1
    - 이진분류 문제의 출력층에서 주로 사용됨.
    
![image](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/5aebee68-3d61-492b-8e5d-aa54ed40ef6f)
    
- **하이퍼볼릭탄젠트 (Hyperbolic Tangent, tanh)**
    - 출력범위: -1~1
    
![image](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/afc958bc-2507-4071-8054-322346d5f994)

    
- **ReLU (Rectified Linear Unit)**
    - 출력범위: 입력이 0 이상이면 그대로, 0 미만이면 0.
    - 문제점: 0 미만이면(음수면) 뉴런이 죽을 수 있음

![image](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/a9b1411a-cde9-4c39-8ecf-2e4558f8c56e)


- **Leaky ReLU**
    - ReLU의 문제점(dying relu)을 해결하기 위해 등장함.
    - 따라서 ReLU와 다르게 0 미만이면 작은 음수 값을 보냄.

![image](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/07df8353-a4f2-4497-a47e-cab5d379fa3a)

- **그래서 왜 활성화 함수를 사용해야 되는데?**
    - 대부분의 문제들은 직선으로 풀 수 없는 복잡한 문제들임. 예를 들어, 손글씨를 인식하거나, 사진 속 얼굴을 찾는 일들은 단순한 선형 함수로는 해결할 수 없음😢 왜냐면 신경망이 아무리 깊어도 복잡한 패턴을 배우지 못하기 때문임. 즉, 레이어가 아무리 많아도 의미가 없음..
    - 근데 활성화 함수는 레이어마다 다른 패턴을 배울 수 있어서 복잡한 문제들을 풀 수 있도록 도와줌. 즉, 비선형성을 도입해서 신경망이 더 복잡한 패턴을 학습할 수 있게 하는 것.

### 📌 순전파, 역전파, 경사하강법

- **순전파**
    - 첫번째 레이어
        - $Z^{[1]}=W^{[1]}X+b^{[1]}$
        - $A^{[1]}=activation(Z^{[1]})$
    - 두번째 레이어
        - $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
        - $Ŷ =activation(Z[2])$
- 비용 함수 계산
- **역전파**
    - $dZ^{[2]}=\hat Y−Y$
    - $dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T}$
    - $db^{[2]} = \frac{1}{m}ΣdZ^{[2]}$
    - $dA^{[1]} = W^{[2]T}dZ^{[2]}$
    - $dZ^{[1]}=dA^{[1]}⋅activation′(Z^{[1]})$
    - $dW[1]=\frac{1}{m}​dZ^{[1]}X^{T}$
    - $db^{[1]} = \frac{1}{m}∑dZ^{[1]}$
- **경사하강법**
    - $W^{[1]}=W^{[1]}−αdW^{[1]}$
    - $b^{[1]}=b^{[1]}−αdb^{[1]}$
    - $W^{[2]}=W^{[2]}−αdW^{[2]}$
    - $b^{[2]}=b^{[2]}−αdb^{[2]}$
