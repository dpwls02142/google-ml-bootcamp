# 2주차
### 📌 로지스틱 회귀

로지스틱 회귀는 *이진 분류에서 사용되는 알고리즘임. *이진 분류란 레이블이 2가지로 분류되는 것을 의미함.

예를 들어 입력 데이터(x)는 고양이 이미지며, 출력 데이터(y)는 입력한 이미지가 고양이면 1을 출력하고, 고양이가 아니면 0을 출력하는 모델을 만든다고 가정해보자.

이미지를 컴퓨터에서 저장하기 위해선 R(빨강),G(초록),B(파랑) 세 개로 분류된 행렬을 사용하는데, 각각의 *픽셀은 채도값을 나타냄. *픽셀(화소) == 이미지를 구성하는 최소 단위

즉, 컬러 채널의 64*64 이미지를 학습시키는데 필요한 차원의 수는(특징 벡터의 차원은) 64(가로)*64(세로)*3(채널수)이 될 것. (이는 n 또는 nx로 표현됨)

- 하나의 훈련 표본은 (x,y)쌍으로 표기함.
    - x는 x차원을 갖는 입력 특징 벡터
    - y는 0 또는 1을 갖는 레이블 → ~~이진 분류니까~~
- **m:** 훈련 세트(여러 훈련 표본으로 구성된 것). (m==m_train)
- **X(입력)**: nx*m 차원의 행렬로, nx(==n)는 특징 벡터의 차원.
- **Y(출력)**: 1*m 차원의 행렬로, 각 훈련 예제의 레이블을 포함.

cf.) 즉, **특징 벡터**라는건 데이터에서 추출한 특정 정보를 수치로 표현한 것을 의미함. 예를 들어, 고양이와 개를 구분하는 이미지 분류 작업에서는 각 이미지를 나타내는 특징 벡터를 생성할 수 있는데. 이 특징 벡터는 이미지의 픽셀 값, 색상 히스토그램, 텍스처 정보 등을 포함할 수 있음. 또는 고객의 구매 패턴을 분석하는 경우, 각 고객은 특정 상품의 구매 횟수, 방문 빈도, 나이, 성별 등을 특징 벡터로 표현할 수 있음.

---

### 📌 방정식

- **선형 방정식 $Ŷ = WX + B$**
    - Ŷ는 Y의 추정치(예측값, 종속변수)
    - W는 가중치 벡터
    - X는 입력 특징 벡터
    - B는 편향

cf.) 선형 방정식은 예측값이 연속적인 값을 가질 때 주로 사용됨. 연속적인 값이란건, 예를 들어 집값이나 온도와 같이 실수로 표현되는 것을 의미함.

그렇담 로지스틱 회귀 방정식은 선형 방정식에 시그모이드 함수를 적용하여 출력값을 0과 1 사이의 확률로 변환한 것임.

- **로지스틱 회귀 방정식** $Ŷ =σ(W^{T}X+B)$
    - σ는 **시그모이드 함수**로, $σ(z)= \frac{1}{1+e^{−z}}​$
    - cf.) 시그모이드 함수?
    - 입력 값을 0과 1 사이의 값으로 변환하는 비선형 함수를 의미함. 즉, 입력 z가 양수이면 1에 가까워지고, 음수이면 0에 가까워지는 S 형태의 곡선을 가짐. 
    ![image](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/a58565d5-8367-4289-8800-1b01c895be0b)

즉, 로지스틱 회귀에서의 목표는 매개변수 W와 B를 활용해 Ŷ이 1일 가능성에 대한 좋은 추정치를 내는 것이라 할 수 있음.

- **cf.) 왜 가중치 벡터를 전치행렬 할까?**

![1](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/ad91322d-5ea9-4696-b29a-c5e84f62142e)

---

### 📌 손실, 비용 함수

- **손실함수 (loss function)**
    - 모델의 성능을 평가하는데 사용되는 함수. 즉, 모델이 예측한 값과 실제 값 간의 차이를 나타내며 이 차이를 최소화하는 것이 모델 학습에서의 목표라 할 수 있음.
    - 그럼 어떻게 하면 최소화를 할 수 있을까? 모델의 파라미터 (W와 B)를 조정하면 됨.
- **비용함수 (cost function)**
    - 모델이 얼마나 잘 작동하는 지를 나타내는 지표로, 비용함수 값이 낮을 수록 모델이 예측한 값이 실제 값과 가깝다는 것을 의미함.

![2](https://github.com/dpwls02142/google-ml-bootcamp/assets/130109502/fa74278c-9e04-4627-b88d-c03f10292431)


- **cf.) 손실함수와 비용함수의 차이점**

손실함수는 개별 데이터 포인트에 대해 예측 오차를 측정하는 것이고, 비용함수는 모든 데이터 포인트에 대해 예측 오차를 측정하는 것이라 할 수 있음. 

왜냐? 비용함수 수식을 보면, $J(W,b)=\frac{1}{m}∑^m_{i=1}
L(y(i),ŷ((i))$ 로 표현되는데 이 때$L(y(i),ŷ(i))$는 각 데이터 포인트에 대한 손실함수를 의미함. 시그마를 사용해 1부터 m까지의 손실함수를 합산하니 비용함수는 전체적인 예측 오차를 측정 한다고 해석할 수 있음.

---

### 📌 경사 하강법

- 손실 함수를 최소화하기 위해 사용되는 알고리즘으로 W와 B를 업데이트(조정)하는 방법을 의미함.
- 손실 함수의 기울기(gradient)를 계산하여 모델의 파라미터를 업데이트하는데,
- 업데이트 방법은 아래 수식처럼 $α$(학습률, learning rate)와 기울기의 곱으로 이뤄짐.

$$
W:=W−α \frac{∂J(W,B)}{∂W}
$$
$$
B:=B−α \frac{∂J(W,B)}{∂B}
$$

- learning rate가 너무 작으면 속도가 느리고 너무 크면 제대로 작동을 안 할 수 있음.
    - 너무 작으면
        - 파라미터 업데이트도 적게되니까 최적에 도달하는데 시간이 오래 걸리게 됨
        - local optimum에 갇힐 수 있음. 이에 갇히면 학습과정이 중단 될 수도 있음.
        - cf.) local optimum? 
    - 너무 크면
        - 손실 함수 값이 지속적으로 증가(즉, 최적 값을 찾지 못하고 반대 방향으로 이동하게 되어 손실이 계속 증가하는 상태. 발산이라고 함) 하거나,
        - 파라미터값이 불규칙하게 왔다갔다하여 손실함수값이 불안정적일 수 있음.(진동이라고 함)
