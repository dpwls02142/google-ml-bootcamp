# 1주차

### 📌 훈련 데이터, 검증 데이터, 테스트 데이터

- **훈련 데이터 (Traning)**
    - 모델의 학습에 사용되는 데이터
- **검증 데이터 (Validation, 개발 데이터)**
    - 모델의 성능을 중간에 평가하고 튜닝하는데 사용되는 데이터
    - 과적합과 하이퍼파라미터를 최적화하기 위해 사용됨.
- **테스트 데이터 (Test, 시험 데이터)**
    - 모델을 성능을 평가할 때 사용되는 데이터
    - 따라서 훈련과 검증 과정에서 한 번도 본 적이 없는 데이터여야함. 즉, 모델의 일반화 능력을 측정하는 지표라 할 수 있음.
- 보통은 훈련 데이터와 개발 데이터를 7:3의 비율로 나눠서 학습 시켰음. → **전체 데이터셋이 작다면 이 비율이 더 나음.**
- 그러나 방대한 양의 데이터가 존재하는 현대의 빅데이터 시대에선 **개발 데이터의 비율이 더 줄어들고 있음**
    - 왜? 개발 데이터의 목표는 다양한 알고리즘을 테스트하고 어떤 알고리즘이 잘 작동되는 지를 확인 하는 것이기 때문. 즉, 특정 알고리즘에서만 잘 작동되는걸 보기 위해서가 아님.
    - 예를 들어 전체 데이터셋이 1,000,000개가 있다고 가정하자. 이 때 개발 데이터는 10,000개, 테스트 데이터는 10,000개가 필요하면 각각 전체 데이터 셋의 1%에 해당함. 따라서 훈련 데이터셋은 나머지 전체 98%를 차지함.
- 근래엔 훈련 데이터와 개발 데이터가 다른 경우도 있음. 예를 들어, 사용자가 입력한 이미지가 고양이인지 아닌지를 판단하는 모델을 만든다고 하자. 이 때 훈련 데이터는 인터넷에서 긁어온 😺 사진이고, 개발 데이터는 사람이 촬영한 😺 사진임. 물론 사진 자체는 동일한 이미지일 수 있지만 해상도라든지, 화질이 다를 수도 있잖음. 이런걸 훈련 데이터와 개발 데이터가 다르다고 하는 것.
- 근데 개발 데이터랑 테스트 데이터는 동일 분포에서 나온게 좋음. *왜?* 모델의 성능을 일관되게 평가하고, 과적합을 방지하기 위해서.
- 즉.. 훈련 데이터와 개발 데이터는 달라도 되지만, 개발 데이터와 테스트 데이터는 동일한 배포판에서 나온게 좋다!

### 📌 편향(bias) & 분산(variance)

- **편향이란?** 예측값이 한 쪽으로 치우쳐져 있는 정도.
    - 편향이 높다는건 예측값과 정답이 멀리있다는거(학습을 잘 못함)
    - 편향이 낮다는건  예측값과 정답이 가까이 있다는거(학습이 잘 됨)
- **분산이란?** 예측값이 퍼져있는 정도.
    - 분산이 낮다는건 예측값들끼리 몰려있다는거
    - 분산이 높다는건 예측값들끼리 흩어져있다는거
- 즉, 편향이 높으면 under fitting이 발생하고, 분산이 높으면 over fitting이 발생
    - under fitting?
        
        모델이 학습 오류를 줄이지 못하는 상황. 이미 있는 train set도 학습을 제대로 못한 상태임.
        
    - over fitting?
        
        train set에만 존재하는 feature(noise)들을 과하게 학습 시켜서 train set에서만 잘 굴러가지도록 만든 것을 말함.
        
- bayes error

### 📌 Regularization

<aside>
💡 목적: overfitting을 방지하여 test set에서 좋은 성능을 내게 하기 위함.

</aside>

> Weight Regularization
> 
- weight값이 크면 overfitting이 발생할 가능성이 큼. *왜?* 모델이 train set에 너무 특화되기 때문. weight는 활성화 값을 결정하는데 중요한 역할을 함. *왜?* bias와 달리 행렬형태로 이뤄져 있으니까 활성화값에 더 큰 영향을 줄 수 밖에 없음.
- 결론~ weight가 크면 작은 입력 변화에도 출력에 큰 영향을 줄 수 있어서 모델이 train set의 noise까지 학습하게 만들 수 있음. 이러면 이제 overfitting이 발생하게 되는거고, 그래서 weight값을 Regularization하는거.
- 그래서  weight 값을 어떻게 규제(Regularization)하는데? Norm을 활용해서 규제할 수 있음. 이에는 L1 정규화랑 L2 정규화가 있음.
    - L1 Regularization: 가중치의 절대값 합(L1 Norm)을 비용함수에 추가. 가중치 중 일부를 0으로 만듦
    - L2 Regularization: 가중치의 제곱합(L2 Norm)을 비용함수에 추가. 모든 가중치를 작게 만듦

> Drop out
> 
- 학습을 할 때 무작위로 일부 뉴런을 꺼버리는 것. 즉, 학습하는 과정에서 랜덤으로 일부 뉴런을 사용하지 않도록해서 네트워크가 너무 특정 패턴에만 의존을 안 하도록 만드는 것임.
- 예를 들어, A랑 B 축구팀이 있는데 A팀은 항상 동일한 에이스 선수들로만 구성해서 경기를 진행함. 근데 이럼 이 중에 한 명이라도 빠지면 팀이 평소보다 약해질 수 있잖음. 근데 B팀은 매번 경기 때마다 선수를 랜덤하게 선별함. 그래서 어떤 상황에서도 잘 대응할 수 있도록 훈련하는 것..

> Data augmentation
> 


- 학습 이미지의 개수를 늘리는 것이 아닌, 학습시마다 개별 원본 이미지를 변형해서 학습하는 것

> Early stopping
> 
- 반복 수행을 할 수록 weight의 값은 점차 늘어날 것, 이 때 w의 비율이 중간 정도 되는 시점에서 중지하는 것.

### 📌 Normalizing Inputs

- 신경망에 입력되는 데이터의 스케일을 조정하는 과정. 주로 각 특성(feature)이 평균이 0이고 표준편차가 1이 되도록 변환함. 입력값이 너무 크거나 작으면 기울기 소실(gradient vanishing)이나 폭주(exploding)를 일으킬 수 있는데, 이를 방지할 수 있음.

### 📌 Gradient Vanishing and Exploding 기울기 소실, 폭주 문제

- 기울기 소실: 역전파(Backpropagation) 과정에서 기울기(gradient)가 층을 거듭할수록 점점 작아져서 거의 0에 가까워지는 현상.
- 기울기 폭주: 역전파 과정에서 기울기가 층을 거듭할수록 점점 커져서 매우 큰 값이 되는 현상.

### 📌 Gradinet Checking